{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Initial NeuralNet Approach\n",
    "The initial investigation in a neural net Ranker used the [multilayered perceptron classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) to create a model that maps a 1200-dimensional question-solution vector into the probability that the particular solution will be linked to the question. With a dataset of the 5000 most recent queries in the vSphere 5.5 domain, this network resulted in a PCT@5 score of `0.3752`, correctly including a linked solution in the within the top 5 ranked solutions for 460 out of 1226 questions in the test set. For comparison, the current production version of Ranker for the same domain has a PCT@5 score of `0.433`, while the baseline has a PCT@5 score of `0.1955`. Two things worth noting is that the PCT@5 score for the initial RankerNet was a single trial and it was an unoptimized prototype. So while there certainly isn't enough evidence to conclude that RankerNet is production-worthy, it's exciting that the early results are promising and futher optimizations should improve it even further.\n",
    "\n",
    "## Early Problems\n",
    "Unfortunately, SKlearn is not a library optimized for GPU computing. We now have an [Amazon P3](https://aws.amazon.com/ec2/instance-types/p3/) instance with a powerful GPU and we certainly want to leverage that to speed up model training and predicting. \n",
    "\n",
    "## Switching to Tensorflow\n",
    "Instead of SKlearn, this notebook will switch over to [Tensorflow](https://www.tensorflow.org/tutorials/) for the implementation of the neural network. In addition to GPU-compatibility, Tensorflow will give us finer control over the neural network, as we can construct it layer-by-layer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "The data collection will remain the same as before, so we'll need to get all of the functions and reuse them here. Rather than defining them within this notebook, I've moved them over to their own files so they can be neatly imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T16:44:22.539111Z",
     "start_time": "2018-12-07T16:43:36.459400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling 3567 docs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d97b1d7911f4a00a0a3c2af4712dee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Pulling docs:', max=3567)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled 3567 docs\n",
      "Found 115649 queries. Query limit: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de858f28a5e4b9a89e3daecc85cd0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Pulling queries:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached limit\n",
      "Pulled 100 queries, 120 skipped\n",
      "Skipped reasons: Counter({'Linked to docs not included in model': 107, 'Not long enough': 12, 'Non-english': 1})\n"
     ]
    }
   ],
   "source": [
    "from helpers.data_helpers import pull_training_data\n",
    "\n",
    "queries, doc_id2body, question_id2body, skipped = pull_training_data(limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "Just as with the data collection, the vectorization functions have been moved to an external file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T16:42:59.614595Z",
     "start_time": "2018-12-07T16:42:59.574517Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T16:42:41.109120Z",
     "start_time": "2018-12-07T16:42:41.061224Z"
    }
   },
   "outputs": [],
   "source": [
    "from helpers.word2vec import get_model, vectorize_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T22:55:37.526134Z",
     "start_time": "2018-12-06T22:50:48.729359Z"
    }
   },
   "outputs": [],
   "source": [
    "W2V_MODEL = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T16:42:43.510792Z",
     "start_time": "2018-12-07T16:42:43.468383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_body(\"Hello there, how are you?\", W2V_MODEL).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train/Test Dataframes\n",
    "This section will utilize the helper functions to create a dataframe of positive/negative question-solution pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T21:24:10.769285Z",
     "start_time": "2018-12-07T21:24:10.714788Z"
    }
   },
   "outputs": [],
   "source": [
    "from helpers.data_helpers import get_id2vector, prune, split_train_test, create_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T19:09:01.501002Z",
     "start_time": "2018-12-07T19:07:19.369888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling 3567 docs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7623d7b7f2fa482ca43ec23b86a07352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Pulling docs:', max=3567)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled 3567 docs\n",
      "Found 115649 queries. Query limit: 5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0834801ac384c898ad38706c1a65d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Pulling queries:', max=5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached limit\n",
      "Pulled 5000 queries, 7091 skipped\n",
      "Skipped reasons: Counter({'Linked to docs not included in model': 6829, 'Not long enough': 255, 'Non-english': 7})\n"
     ]
    }
   ],
   "source": [
    "queries, doc_id2body, question_id2body, skipped = pull_training_data(limit=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T19:12:57.348600Z",
     "start_time": "2018-12-07T19:12:04.476217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128482e6bfae47a99ab447bb75b83952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Vectorizing:', max=8566)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "id2vector, skipped_docs, skipped_questions = get_id2vector(doc_id2body, question_id2body, W2V_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T19:12:57.405720Z",
     "start_time": "2018-12-07T19:12:57.352457Z"
    }
   },
   "outputs": [],
   "source": [
    "pruned_queries = prune(queries, skipped_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T21:27:10.181882Z",
     "start_time": "2018-12-07T21:24:14.396241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3670 queries in train set\n",
      "1224 queries in test set\n",
      "Date pivot point: 2018-09-21T00:26:49Z\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe35825af0d44cab78c985d34278f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Adding queries:', max=3670)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ab0a2537b54519bf9065e8cce0d4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Adding queries:', max=1224)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "date_pivot, train, test = split_train_test(pruned_queries, date_split_proportion=0.75)\n",
    "print(\"%d queries in train set\" % len(train))\n",
    "print(\"%d queries in test set\" % len(test))\n",
    "print(\"Date pivot point:\", date_pivot)\n",
    "\n",
    "train_df = create_dataframe(train, n=200)\n",
    "test_df = create_dataframe(test, n=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "In order to ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed/Memory Profiling of Adding Vectors\n",
    "The initial approach was to iterate across each row in the dataframe, create the concaatenated vector, and then add that as a column to the original dataframe. This section investigates the use of `iterrows()` vs. `apply()` to creating the vectors. __Spoiler alert__: I end up changing the approach entirely and saving the vectors as their own dataframe, where each element has its own column. See the end of this section for the speed and memory benefits of that approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T20:14:38.264267Z",
     "start_time": "2018-12-07T20:14:38.202451Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "from helpers.data_helpers import concatenate_vectors\n",
    "\n",
    "\n",
    "def add_vectors_apply(dataframe, id2vector):\n",
    "    print(f\"Adding {len(dataframe)} vectors.\")\n",
    "    dataframe[\"vector\"] = dataframe.apply(lambda row: concatenate_vectors(id2vector[row[\"question_id\"]], id2vector[row[\"solution_id\"]]), axis=1)\n",
    "    return dataframe  \n",
    "\n",
    "def add_vectors_iterrows(dataframe, id2vector):\n",
    "    progress_bar = IntProgress(min=0, max=(len(dataframe) - 1), description='Adding vectors:', bar_style='info')\n",
    "    display(progress_bar)\n",
    "    print(f\"Adding {len(dataframe)} vectors.\")\n",
    "    vectors = []\n",
    "    for count, (index, row) in enumerate(dataframe.iterrows()):\n",
    "        if count % 10000 == 0:\n",
    "            progress_bar.value = count\n",
    "        vectors.append(concatenate_vectors(id2vector[row[\"question_id\"]], id2vector[row[\"solution_id\"]]))\n",
    "            \n",
    "            \n",
    "    progress_bar.value = len(dataframe) - 1\n",
    "    dataframe[\"vector\"] = pd.Series(vectors, index=dataframe.index)\n",
    "    return dataframe   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T19:35:41.021193Z",
     "start_time": "2018-12-07T19:29:09.115408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 245132 vectors.\n",
      "Adding 245132 vectors.\n",
      "Adding 245132 vectors.\n",
      "Adding 245132 vectors.\n",
      "Adding 245132 vectors.\n",
      "Adding 245132 vectors.\n",
      "Adding 245132 vectors.\n",
      "Adding 245132 vectors.\n",
      "48.9 s ± 153 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "add_vectors_apply(test_df, id2vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T19:57:24.305273Z",
     "start_time": "2018-12-07T19:50:54.217507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb56b5642ee4a9e98927861486e4047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Adding vectors:', max=245131)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 245132 vectors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f310586394944f5b26b836557d3cd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Adding vectors:', max=245131)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 245132 vectors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8597a6b9b64e65a9570261c503fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Adding vectors:', max=245131)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 245132 vectors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03a1846b33e4620a8ccd97c51032f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Adding vectors:', max=245131)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 245132 vectors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7447c3a3194d02861522d81b572975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Adding vectors:', max=245131)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 245132 vectors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63223e6249242a1ae1097dddb23e55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Adding vectors:', max=245131)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 245132 vectors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c718e6935141d2a4a902832258fe09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Adding vectors:', max=245131)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 245132 vectors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67aaa8412fb46a491fa9879836f5dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Adding vectors:', max=245131)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 245132 vectors.\n",
      "48.7 s ± 356 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "add_vectors_iterrows(test_df, id2vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would have expected the `apply()` approach to be at least slightly faster than `iterrows()`. Either way, I'll try a new approach where instead of creating a function that takes a single question vector and a single solution vector, the function will take in a list of question vectors and a list of solution vectors. This bypasses the loop across all of the rows of the dataframe, as instead we can just pass the entire columns of the `question_id`s and `solution_id`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T20:34:03.730962Z",
     "start_time": "2018-12-07T20:34:03.680841Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_vectors(dataframe, id2vector):\n",
    "    return pd.Series(np.concatenate([itemgetter(*dataframe[\"question_id\"].tolist())(id2vector),\n",
    "                                     itemgetter(*dataframe[\"solution_id\"].tolist())(id2vector)],\n",
    "                                    axis=1).tolist(), \n",
    "                     index=dataframe.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T20:29:19.120840Z",
     "start_time": "2018-12-07T20:27:29.836386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.7 s ± 81.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "add_vectors(test_df, id2vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a winner! Looping over all of the rows is quite expensive, so writing the function in a way that can take in an array of all of the values, rather than a single row's value, provides a _considerable_ boost in performance. However, we won't be using any of those preceding functions. If the vector is stored in a Dataframe as a single value in a column, it has a type of `object`, whereas if we create a new dataframe for the vectors where each element has its own column, then they can be stored more efficiently as `float32`s. \n",
    "\n",
    "To get a concrete example, I'll create a DataFrame where each vector is stored in a single column as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T21:32:33.380708Z",
     "start_time": "2018-12-07T21:32:16.849193Z"
    }
   },
   "outputs": [],
   "source": [
    "exp_df = pd.DataFrame(add_vectors(test_df, id2vector),\n",
    "                      index=np.arange(len(test_df)),\n",
    "                      columns=[\"vector\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how much memory it requires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T21:32:52.523958Z",
     "start_time": "2018-12-07T21:32:52.412959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 245132 entries, 0 to 245131\n",
      "Data columns (total 1 columns):\n",
      "vector    245132 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 2.2 GB\n"
     ]
    }
   ],
   "source": [
    "exp_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, when the vectors are left as arrays, they're saved as `object`s and this one in particular takes 2.2 GB of data. \n",
    "\n",
    "## Creating the Vector Datarame\n",
    "Now let's do the alternative approach of creating a new dataframe for the vectors where each element has its own column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T20:48:53.792785Z",
     "start_time": "2018-12-07T20:48:53.742861Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vector_dataframe(dataframe, id2vector):\n",
    "    return pd.DataFrame(np.concatenate([itemgetter(*dataframe[\"question_id\"].tolist())(id2vector),\n",
    "                                         itemgetter(*dataframe[\"solution_id\"].tolist())(id2vector)],\n",
    "                                        axis=1),\n",
    "                         index=np.arange(len(dataframe)),\n",
    "                         columns=np.arange(1200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T20:55:44.293988Z",
     "start_time": "2018-12-07T20:55:42.456943Z"
    }
   },
   "outputs": [],
   "source": [
    "test_vector_df = get_vector_dataframe(test_df, id2vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T20:55:46.223465Z",
     "start_time": "2018-12-07T20:55:46.060653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 245132 entries, 0 to 245131\n",
      "Columns: 1200 entries, 0 to 1199\n",
      "dtypes: float32(1200)\n",
      "memory usage: 1.1 GB\n"
     ]
    }
   ],
   "source": [
    "test_vector_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing the overhead of saving them as `object`s and explicitly storing them as `float32`s, the size of the vectors decreased by 50%. Not bad! Additionally, it took 1.84 seconds to create the dataframe, while the `iterrows()` and `apply()` approaches took ~50 seconds. We can still easily retrieve the vector as an array with the `.values` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T21:46:34.645848Z",
     "start_time": "2018-12-07T21:46:34.599169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00908015, 0.00347431, 0.02504319, ..., 0.3046875 , 0.40820312,\n",
       "       0.578125  ], dtype=float32)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vector_df.iloc[0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Train/Test Vector Dataframes\n",
    "To conclude this section, I'll vectorize the training and testing dataframes, creating a `test_vector_df` and `train_vector_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T21:48:01.825889Z",
     "start_time": "2018-12-07T21:48:00.027922Z"
    }
   },
   "outputs": [],
   "source": [
    "test_vector_df = get_vector_dataframe(test_df, id2vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T21:48:07.384595Z",
     "start_time": "2018-12-07T21:48:01.829434Z"
    }
   },
   "outputs": [],
   "source": [
    "train_vector_df = get_vector_dataframe(train_df, id2vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TenseRankerFlowNet\n",
    "With the data loaded in and vectorized, it's time to leverage Tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T22:04:30.664701Z",
     "start_time": "2018-12-07T22:04:30.606650Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1200, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(300, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(150, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(50, activation=tf.nn.relu)\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "257px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
